"""
Script to upload dish embeddings to Supabase pgvector table.

This script reads the embeddings generated by Clean-dish-list project
and uploads them to the Supabase dish_embeddings table.

Usage:
    python scripts/upload_embeddings_to_supabase.py
"""

import os
import sys
import pandas as pd
import numpy as np
from dotenv import load_dotenv

# Add parent directory to path for imports
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.core.supabase_client import get_supabase_client

# Load environment variables
load_dotenv()

# Paths to embedding files (adjust if needed)
EMBEDDINGS_DIR = os.path.join(
    os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))),
    "Clean-dish-list",
    "embeddings"
)
PARQUET_FILE = os.path.join(EMBEDDINGS_DIR, "recipes.bge-m3.parquet")
CSV_FILE = os.path.join(
    os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))),
    "Clean-dish-list",
    "recipes.cleaned.menu.csv"
)


def upload_embeddings():
    """Upload embeddings from parquet file to Supabase"""

    print("Loading embeddings from parquet file...")

    # Check if file exists
    if not os.path.exists(PARQUET_FILE):
        print(f"Error: Parquet file not found at {PARQUET_FILE}")
        print(f"Please ensure embeddings are generated in Clean-dish-list project")
        return False

    # Read parquet file
    df = pd.read_parquet(PARQUET_FILE)
    print(f"Loaded {len(df)} embeddings")

    # Check required columns
    required_cols = ['embedding', 'title', 'text_for_embedding']
    missing_cols = [col for col in required_cols if col not in df.columns]
    if missing_cols:
        print(f"Error: Missing required columns: {missing_cols}")
        print(f"Available columns: {list(df.columns)}")
        return False

    # Also load the CSV to get the name_opt, description, and type
    if os.path.exists(CSV_FILE):
        print(f"Loading metadata from CSV file...")
        csv_df = pd.read_csv(CSV_FILE)

        # Merge based on title or another common column
        # Assuming both have 'title' column
        if 'title' in csv_df.columns:
            df = df.merge(
                csv_df[['title', 'name_opt', 'description', 'type']],
                on='title',
                how='left'
            )
        else:
            print("Warning: CSV does not have 'title' column, using generated name_opt")
            # Generate name_opt from title
            df['name_opt'] = df['title'].str.lower().str.replace(' ', '-')
            df['description'] = df.get('text_for_embedding', '')
            df['type'] = 'food'
    else:
        print(f"Warning: CSV file not found at {CSV_FILE}")
        print("Generating metadata from available data...")
        # Generate name_opt from title
        df['name_opt'] = df['title'].str.lower().str.replace(' ', '-')
        df['description'] = df.get('text_for_embedding', '')
        df['type'] = 'food'

    # Fill NaN values
    df['name_opt'] = df['name_opt'].fillna(df['title'].str.lower().str.replace(' ', '-'))
    df['description'] = df['description'].fillna('')
    df['type'] = df['type'].fillna('food')

    print(f"Sample data:")
    print(df[['name_opt', 'title', 'type']].head())

    # Connect to Supabase
    print("\nConnecting to Supabase...")
    supabase = get_supabase_client()

    # Prepare records for upload
    print("\nPreparing records for upload...")
    batch_size = 100
    total_uploaded = 0
    errors = 0

    for i in range(0, len(df), batch_size):
        batch = df.iloc[i:i+batch_size]

        records = []
        for _, row in batch.iterrows():
            # Convert embedding from list to postgres vector format
            embedding = row['embedding']
            if isinstance(embedding, str):
                # If it's a string, convert to list
                embedding = eval(embedding)
            elif isinstance(embedding, np.ndarray):
                embedding = embedding.tolist()

            records.append({
                'name_opt': row['name_opt'],
                'title': row['title'],
                'description': row['description'],
                'type': row['type'],
                'embedding': embedding
            })

        # Upload batch
        try:
            response = supabase.table('dish_embeddings').upsert(
                records,
                on_conflict='name_opt'
            ).execute()

            total_uploaded += len(records)
            print(f"Uploaded batch {i//batch_size + 1}/{(len(df)-1)//batch_size + 1} "
                  f"({total_uploaded}/{len(df)} records)")

        except Exception as e:
            print(f"Error uploading batch {i//batch_size + 1}: {str(e)}")
            errors += 1

            # Try uploading one by one for this batch to identify problematic records
            for record in records:
                try:
                    supabase.table('dish_embeddings').upsert(
                        [record],
                        on_conflict='name_opt'
                    ).execute()
                    total_uploaded += 1
                except Exception as record_error:
                    print(f"  Error with record '{record['name_opt']}': {str(record_error)}")
                    errors += 1

    print(f"\n{'='*60}")
    print(f"Upload complete!")
    print(f"Total records: {len(df)}")
    print(f"Successfully uploaded: {total_uploaded}")
    print(f"Errors: {errors}")
    print(f"{'='*60}")

    return errors == 0


if __name__ == "__main__":
    print("=" * 60)
    print("Dish Embeddings Upload Script")
    print("=" * 60)

    success = upload_embeddings()

    if success:
        print("\n✓ All embeddings uploaded successfully!")
        sys.exit(0)
    else:
        print("\n✗ Some embeddings failed to upload. Check errors above.")
        sys.exit(1)
